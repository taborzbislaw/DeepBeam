{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on simulated data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interpn\n",
    "import os\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import config\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all simulated profiles for a regular grid of primary beam parameters, for fields 3x3, 10x10, and 30x30\n",
    "\n",
    "dataPoints = [(str(e),str(se),str(s),str(an)) for e in config.simulatedEnergies for se in config.simulatedEnergyDispersions for s in config.simulatedSourceSizes \n",
    "              for an in config.simulatedAngularDivergences]\n",
    "\n",
    "random.seed(config.SEED)\n",
    "random.shuffle(dataPoints)\n",
    "\n",
    "profiles = utils.readProfiles(config.profileDIR,dataPoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 4) <class 'numpy.float64'>\n",
      "(300, 6, 495)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create means and goals for the selected part (config.FRACTION) of all simulated data\n",
    "\n",
    "trainProfiles = []\n",
    "for field in range(3):\n",
    "    trainProfiles.append(profiles[field][:(int)(profiles[field].shape[0]*config.FRACTION)])\n",
    "\n",
    "goals = np.asarray(dataPoints[:(int)(len(dataPoints)*config.FRACTION)],dtype=np.float)\n",
    "\n",
    "means = []\n",
    "for field in range(3):\n",
    "    means.append(np.mean(trainProfiles[field],0))\n",
    "\n",
    "diffTrain = []\n",
    "for field in range(3):\n",
    "    diffTrain.append(trainProfiles[field] - np.stack([means[field] for _ in range(trainProfiles[field].shape[0])]))\n",
    "\n",
    "np.save(config.modelDIR + config.meansFileName,np.asarray(means))\n",
    "np.save(config.modelDIR + config.goalsFileName,goals)\n",
    "\n",
    "print(goals.shape,type(goals[0,0]))\n",
    "print(trainProfiles[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0 298 0.9884003797799471 [0.97495119 0.00993918 0.00351001]\n",
      "1 1 166 329 0.9894553033022359 [0.85234076 0.12605318 0.01106136]\n",
      "1 3 159 336 0.986504395353545 [0.8176412  0.12863054 0.04023265]\n",
      "2 0 0 297 0.981171087343056 [0.87828978 0.09371993 0.00916138]\n",
      "2 1 65 430 0.9931304311772537 [0.94832709 0.03923426 0.00556908]\n",
      "2 3 50 445 0.9928726172870717 [0.92280423 0.05462276 0.01544562]\n",
      "(300, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run PCA to reduce dimensionality of the data; write PCA models to files\n",
    "# PCA - https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "\n",
    "trainFeatures = []\n",
    "\n",
    "for nfield,(field,Ranges) in enumerate(zip(config.analyzedProfiles,config.analyzedRanges)):\n",
    "    if field != None:\n",
    "        for profile,Range in zip(field,Ranges):\n",
    "            pca = PCA(n_components=config.numbefOfPCAFeatures)\n",
    "            X = diffTrain[nfield][:,profile,Range[0]:Range[1]]\n",
    "            pca.fit(X)\n",
    "            X_projected = pca.transform(X)\n",
    "            trainFeatures.append(X_projected)\n",
    "            pcaName = config.modelDIR + 'PCA_' + str(nfield) + '_' + str(profile) + '_.pkl'\n",
    "            pickle.dump(pca, open(pcaName,\"wb\"))\n",
    "\n",
    "            print(nfield,profile,Range[0],Range[1],np.sum(pca.explained_variance_ratio_),pca.explained_variance_ratio_)\n",
    "            \n",
    "X_train = np.stack(trainFeatures)\n",
    "X_train = np.swapaxes(X_train,1,0)\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1]*X_train.shape[2]))  \n",
    "print(X_train.shape)\n",
    "\n",
    "np.save(config.modelDIR + config.trainFeaturesFileName,X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 18) (300, 4)\n",
      "0.9890769898754829\n",
      "0.0\n",
      "0.99753602689454\n",
      "0.9807872079755784\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run regression - best model is selected, based on 5 cross-validation using the training data\n",
    "# The best models are saved to files and used later for testing\n",
    "# SVR - https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "# Model selection - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "featuresFileName = config.modelDIR + config.trainFeaturesFileName\n",
    "goalsFileName = config.modelDIR + config.goalsFileName\n",
    "\n",
    "X_train = np.load(featuresFileName)\n",
    "y_train = np.load(goalsFileName)\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "#można dodać sample_weights, aby wskazać wagę poszczególnych profili\n",
    "\n",
    "parameters = {'C':np.arange(0.5,20.5,2.5)\n",
    "              ,'epsilon':[0.01,0.1,0.5,1]}\n",
    "\n",
    "for param in config.trainingGoals:\n",
    "    svr = SVR()\n",
    "    clf = GridSearchCV(svr, parameters)\n",
    "    clf.fit(X_train,y_train[:,param])\n",
    "#    print(clf.best_params_)\n",
    "    print(clf.score(X_train,y_train[:,param]))\n",
    "    #print(clf.best_estimator_.score)\n",
    "    modelName = config.modelDIR + 'SVR_' + str(param) + '_.pkl'\n",
    "    pickle.dump(clf, open(modelName,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
